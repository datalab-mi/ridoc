# -*- coding: utf-8 -*-
"""Fichier de backend pour gérer les requettes elasticsearch"""

import elasticsearch
from elasticsearch import Elasticsearch
import os
import base64
import json
import logging
import datetime
from os.path import expanduser
from os import environ
import unicodedata
from datetime import date
from copy import deepcopy
from pathlib import Path
from shutil import copyfile
import pandas as pd
from tools.converter import pdf2json, save_json


def empty_tree(pth: Path):
    for child in pth.iterdir():
        if child.is_file():
            child.unlink()
        else:
            empty_tree(child)


#On établit une connection
es = Elasticsearch([{'host': 'elasticsearch', 'port': '9200'}])
indices = elasticsearch.client.IndicesClient(es)

def simple_request(nom_index):
    request = {
        "query": {
            "match_all" : {}
            }
    }

    #D = es.search(index=str(nom_index), size=15, body=request)
    D = es.search(index=str(nom_index_prop), size=15, body=request)
    return D['hits']['hits']

def build_query(req:str, index_name:str,
        glossary_file=None, expression_file=None) -> dict:
    """
    Build the body of the query
    Args:
        index_name : Le nom de l'index où on effectue la recherche
        req : la requête entrèe par l'utilisateur
        expression_file : Le lien pour la liste des expressions clés analysée
        glossary_file : Fichier glossaire
    Returns:
        dict
    """
    from_date, to_date, list_author = None, None, []
    # process gloassary
    if glossary_file:
        with open(glossary_file, 'r') as f:
            content = f.read()
        list_glossary = [x.split('=>') for x in str(content).split(
                                '\n') if '=>' in x]

        dic_glossary = {x[0].strip() : x[1].replace(
                        ',', '').strip() for  x  in list_glossary}
        #print(dic_glossary)

        # If user input contains acronym in glossary, add it to req
        for key, val in dic_glossary.items():
            if key in [word.lower() for word in req.split(' ')]:
                req = req + ' ' + val

    if expression_file:
        # process expression
        with open(expression_file, 'r') as f:
            content = f.read()
        list_expression = [x.split('=>')[1] for x in content.split(
                '\n')[:-1] if '=>' in x] # last empty line
        list_expression = [x.replace('_','').strip() for x in list_expression]
    # TODO : process all keyword???
    # Liste_acronyme

    print(req)
    #Au début on va analyser la requête

    body = {'analyzer':"my_analyzer", "text": req}
    analyse = indices.analyze(index= index_name, body = body)

    analyzed = [ananyse_tokens['token'] for ananyse_tokens in analyse["tokens"]]

    lenght_of_request = len(analyzed)

    #------------------------Application du filtre 1 -----------------------------
    T = False
    Bande = False
    """ all keyword???
    Bande = False
    for keyword in Liste_acronyme:
        if keyword in analyzed:
            T = True
            break
    """
    # find expression in the request
    req_expression = []
    for expression in list_expression:
        if expression in analyzed :
            req_expression.append(expression)
            print('req_expression :')
            print(req_expression)
    #import pdb; pdb.set_trace()

    body = { "query": {
                    "bool": {
                        "must": [],
                        "filter": [],
                        "should": [],
                        "must_not": []
                                }
                        },
                    "highlight" : {
                    "pre_tags" : ["<mark>"],
                    "post_tags" : ["</mark>"],
                    "fragment_size" : 300,
                    "number_of_fragments" : 3,
                    "order" : "score",
                    "boundary_scanner" : "sentence",
                    "boundary_scanner_locale" : "fr-FR",
                    "fields":{
                        "content":{}
                        }
                    }
                }


    if len(req.strip()) > 0:
        body["query"]['bool']['must'].append({"simple_query_string": {
                                    "fields" : ["content" , "title"],
                                    "query": req ,
                                    "analyze_wildcard": False
                                                }})

    for key in req_expression:
      body['query']['bool']["should"].append({"match":{
                                      'content' :{
                                          "query" : key,
                                          "boost" : 5
                                                }}})


    if from_date is not None:
        body['query']['bool']["filter"].append({"range" :
                                {"date" : {"gte" : from_date}}
                                             })
    if  to_date is not None:
        body['query']['bool']["filter"].append({"range" :
                                {"date" : {"lte" : to_date}}
                                            })

    for author in list_author:
        author = author.strip()
        if len(author) > 0:
          body['query']['bool']["filter"].append({"match" :
                            {"author" : {"query" : author}}})

    return body, lenght_of_request

def search(req , index_name ,  glossary_file=None, expression_file=None):
    """Fonction qui permet de faire la recherche Elastic dans notre index

    Args:
        index_name : Le nom de l'index où on effectue la recherche
        req : la requête entrèe par l'utilisateur
        expression_file : Le lien pour la liste des expressions clés analysée
        glossary_file : Fichier glossaire
    Output:
        Dictionnaire des résultats de la recherche
    """
    T = False
    Bande = False
    """ all keyword???
    Bande = False
    for keyword in Liste_acronyme:
        if keyword in analyzed:
            T = True
            break
    """
    if req != '':
        body, lenght_of_request = build_query(req,
                            index_name,
                            glossary_file,
                            expression_file)

        D = es.search(index = index_name,
                      body = body,
                      size = 10)
    else:
        lenght_of_request = None
        D = es.search(index = index_name,
                      body = {
                            "query": {
                                "match_all": {}
                            }
                        },
                    size = int(10000)
                    )
    try: #This try is for the case where no match is found
        if not T and D['hits']['hits'][0]["_score"]/lenght_of_request < seuil: #The first filter then the second filter
          Bande = True
    except:
        pass

    return D['hits']['hits'], lenght_of_request , Bande

def corriger(req , nom_index):
  """
    Fonction qui permet de corriger la requête de l'utilisateur
    Arguments:
      nom_index : Le nom de l'index où on effectue la recherche des mots
      req : la requête entrèe par l'utilisateur
    Output:
      Liste des suggestions de correction
"""
  D = es.search(index = nom_index , body = {
  "suggest" : {
   "mytermsuggester" : {
      "text" : req,
      "phrase" : {
         "field" : "Réponse"
       }
    }
  }
  })
  L = []
  for i in range (len(list(D['suggest']['mytermsuggester'][0]['options']))):
    L.append(D['suggest']['mytermsuggester'][0]['options'][i]['text'])
  return L


def create_index(nom_index,
            user_data,
            es_data,
            mapping_file,
            glossary_file=None,
            expression_file=None):


    synonym_file = os.path.join(es_data, 'synonym.txt')
    print(os.path.join(user_data, mapping_file))
    with open(os.path.join(user_data, mapping_file) , 'r' , encoding = 'utf-8') as json_file:
        map = json.load(json_file)
    # Copy glossary and experssion file to elastic search mount volume
    print(map)

    with open(synonym_file, 'w') as outfile:
        if glossary_file:
            print('Use glossary file %s'%glossary_file)
            with open(os.path.join(user_data, glossary_file), 'r') as f1:
                outfile.write(f1.read())
        outfile.write('\n')
        if expression_file:
            print('Use expresion file %s'%expression_file)
            with open(os.path.join(user_data, expression_file), 'r') as f2:
                outfile.write(f2.read())

        else:
            outfile.write()

    map['settings']["analysis"]["filter"]["synonym"].update(
            {"synonyms_path" : os.path.join(es_data, synonym_file)})

    print(map)
    with open(os.path.join(es_data, mapping_file), 'w') as outfile:
        json.dump(map, outfile)

    es.indices.delete(index=nom_index, ignore=[400, 404])
    # create index
    es.indices.create(index = nom_index, body=map)


def inject_documents(nom_index, user_data, pdf_path, json_path,
            meta_path=None):

    no_match = 0

    (Path(user_data) / json_path).mkdir(parents=True, exist_ok=True)
    # empty it in case
    empty_tree(Path(user_data) / json_path)

    for path_document in (Path(user_data) / pdf_path).glob('**/*.pdf'):
        try:
            #path_document = pdf_path / filename
            print('Read %s'%str(path_document))

            index_file(path_document.name, nom_index, user_data, pdf_path, json_path,
                        meta_path)
            print('Document %s just uploaded'% path_document)

        except Exception as e:
            no_match += 1
            print(e)
            #break
            print(20*'*')
            print('Error, cannot upload %s'%path_document)
            print(20*'*')

    print("There is %s documents without metadata match"%no_match)

def index_file(filename, nom_index, user_data, pdf_path, json_path,
            meta_path):
    """
    Index json file to elastic with metadata
    Todo : substitue this function in inject_document
    """
    path_file = Path(user_data) / pdf_path / filename
    data = pdf2json(str(path_file))

    path_meta =  Path(user_data) / meta_path / filename
    path_json =  Path(user_data) / json_path / filename
    path_meta = path_meta.with_suffix('').with_suffix('.json') # replace extension
    path_json = path_json.with_suffix('').with_suffix('.json') # replace extension

    if path_meta.exists():
        with open(path_meta, 'r' , encoding = 'utf-8') as json_file:
            meta = json.load(json_file)
            data.update(meta)

    save_json(data, path_json)

    res = es.index(index = nom_index, body=data , id = filename)
    return res

def delete_file(filename, nom_index):
    try:
        res = es.delete(index = nom_index, id = filename)
        return 200, res
    except elasticsearch.exceptions.NotFoundError as e:
        return e.status_code, e.info
    except:
        print("Unexpected error:", sys.exc_info()[0])
        raise

if __name__ == '__main__':

    NOM_INDEX = 'prod'
    USER_DATA = '/data/user'
    ES_DATA = '/usr/share/elasticsearch/data/extra'

    GLOSSARY_FILE = 'glossaire.txt'
    EXPRESSION_FILE = 'syn_expressions_metier.txt'
    MAPPING_FILE = 'map.json'
    METADATA_FILE = 'METADATA.xlsx'

    PDF_DIR = 'Data_Pdf'
    JSON_DIR = 'Data_Json'

    create_index(NOM_INDEX, USER_DATA, ES_DATA, MAPPING_FILE,
                GLOSSARY_FILE,
                EXPRESSION_FILE)

    inject_documents(NOM_INDEX, USER_DATA, PDF_DIR, JSON_DIR,
            metada_file = METADATA_FILE)
